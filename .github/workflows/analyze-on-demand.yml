name: On-Demand Repository Analysis

on:
  repository_dispatch:
    types: [analyze-repo]
  workflow_dispatch:
    inputs:
      repository:
        description: 'GitHub repository to analyze (owner/repo)'
        required: true
        type: string
      analysis_id:
        description: 'Unique analysis ID for tracking (auto-generated)'
        required: false
        type: string
      branch:
        description: 'Branch to analyze'
        required: false
        type: string
        default: 'main'
      max_files:
        description: 'Maximum files to analyze'
        required: false
        type: string
        default: '500'
      callback_url:
        description: 'URL to POST results to when complete'
        required: false
        type: string

permissions:
  contents: write
  pages: write
  id-token: write
  actions: read

jobs:
  analyze:
    name: Analyze C++ Repository
    runs-on: ubuntu-latest
    
    steps:
    - name: Parse inputs
      run: |
        set -euo pipefail
        # Set default values first
        echo "REPO=" >> $GITHUB_ENV
        echo "BRANCH=main" >> $GITHUB_ENV
        echo "MAX_FILES=500" >> $GITHUB_ENV
        echo "CALLBACK_URL=" >> $GITHUB_ENV
        echo "ANALYSIS_ID=" >> $GITHUB_ENV
        
        # Handle both workflow_dispatch and repository_dispatch
        if [ "${{ github.event_name }}" = "repository_dispatch" ]; then
          echo "REPO=${{ github.event.client_payload.repository }}" >> $GITHUB_ENV
          echo "BRANCH=${{ github.event.client_payload.branch || 'main' }}" >> $GITHUB_ENV
          echo "MAX_FILES=${{ github.event.client_payload.max_files || '500' }}" >> $GITHUB_ENV
          echo "CALLBACK_URL=${{ github.event.client_payload.callback_url }}" >> $GITHUB_ENV
          if [ -n "${{ github.event.client_payload.analysis_id }}" ]; then
            echo "ANALYSIS_ID=${{ github.event.client_payload.analysis_id }}" >> $GITHUB_ENV
          else
            echo "ANALYSIS_ID=$(date +%s)-$(echo $RANDOM)" >> $GITHUB_ENV
          fi
        else
          echo "REPO=${{ github.event.inputs.repository }}" >> $GITHUB_ENV
          echo "BRANCH=${{ github.event.inputs.branch || 'main' }}" >> $GITHUB_ENV
          echo "MAX_FILES=${{ github.event.inputs.max_files || '500' }}" >> $GITHUB_ENV
          echo "CALLBACK_URL=${{ github.event.inputs.callback_url }}" >> $GITHUB_ENV
          
          # Use provided analysis ID or generate one
          if [ -n "${{ github.event.inputs.analysis_id }}" ]; then
            echo "ANALYSIS_ID=${{ github.event.inputs.analysis_id }}" >> $GITHUB_ENV
          else
            echo "ANALYSIS_ID=$(date +%s)-$(echo $RANDOM)" >> $GITHUB_ENV
          fi
        fi

    - name: Checkout CPPCheck Studio
      uses: actions/checkout@v4
      with:
        path: cppcheck-studio

    - name: Setup environment
      uses: actions/setup-node@v4
      with:
        node-version: '18'

    - name: Install dependencies
      run: |
        set -euo pipefail
        sudo apt-get update
        sudo apt-get install -y cppcheck jq
        
        # Build dashboard generator in subshell to avoid directory navigation issues
        (
          cd cppcheck-studio/cppcheck-dashboard-generator
          npm ci
          npm run build
          
          # Force use the local built version
          sudo npm link --force
        )
        
        # Verify the correct version is being used
        echo "Checking dashboard generator version..."
        which cppcheck-dashboard
        cppcheck-dashboard --version || echo "No version info"

    - name: Setup status tracking
      run: |
        set -euo pipefail
        # Create function for status updates
        mkdir -p status_updates
        
        # Set workflow variables for the status function
        export WORKFLOW_RUN_ID="${{ github.run_id }}"
        export WORKFLOW_RUN_URL="${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
        echo "WORKFLOW_RUN_ID=${{ github.run_id }}" >> $GITHUB_ENV
        echo "WORKFLOW_RUN_URL=${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}" >> $GITHUB_ENV
        
        # Create the status function using printf to avoid heredoc issues
        printf '#!/bin/bash\n' > status_updates/update_status.sh
        printf 'update_analysis_status() {\n' >> status_updates/update_status.sh
        printf '    local status=$1\n' >> status_updates/update_status.sh
        printf '    local message=$2\n' >> status_updates/update_status.sh
        printf '    local step=$3\n' >> status_updates/update_status.sh
        printf '    \n' >> status_updates/update_status.sh
        printf '    # Use absolute path to ensure we write to the correct location\n' >> status_updates/update_status.sh
        printf '    STATUS_DIR="$GITHUB_WORKSPACE/status_updates"\n' >> status_updates/update_status.sh
        printf '    mkdir -p "$STATUS_DIR"\n' >> status_updates/update_status.sh
        printf '    \n' >> status_updates/update_status.sh
        printf '    # Create status JSON\n' >> status_updates/update_status.sh
        printf '    cat > "$STATUS_DIR/current_status.json" << EOJSON\n' >> status_updates/update_status.sh
        printf '{\n' >> status_updates/update_status.sh
        printf '  "analysis_id": "${ANALYSIS_ID}",\n' >> status_updates/update_status.sh
        printf '  "repository": "${REPO}",\n' >> status_updates/update_status.sh
        printf '  "status": "${status}",\n' >> status_updates/update_status.sh
        printf '  "step": "${step}",\n' >> status_updates/update_status.sh
        printf '  "workflow_run_id": "${WORKFLOW_RUN_ID}",\n' >> status_updates/update_status.sh
        printf '  "workflow_run_url": "${WORKFLOW_RUN_URL}",\n' >> status_updates/update_status.sh
        printf '  "updated_at": "$(date -u +%%Y-%%m-%%dT%%H:%%M:%%SZ)",\n' >> status_updates/update_status.sh
        printf '  "message": "${message}",\n' >> status_updates/update_status.sh
        printf '  "progress": {\n' >> status_updates/update_status.sh
        printf '    "steps_completed": ${STEPS_COMPLETED:-0},\n' >> status_updates/update_status.sh
        printf '    "total_steps": 5,\n' >> status_updates/update_status.sh
        printf '    "current_step": "${step}",\n' >> status_updates/update_status.sh
        printf '    "files_found": ${FILE_COUNT:-0},\n' >> status_updates/update_status.sh
        printf '    "issues_found": ${ISSUE_COUNT:-0}\n' >> status_updates/update_status.sh
        printf '  }\n' >> status_updates/update_status.sh
        printf '}\n' >> status_updates/update_status.sh
        printf 'EOJSON\n' >> status_updates/update_status.sh
        printf '    \n' >> status_updates/update_status.sh
        printf '    echo "üìä Status Update: ${status} - ${message}"\n' >> status_updates/update_status.sh
        printf '}\n' >> status_updates/update_status.sh
        
        chmod +x status_updates/update_status.sh
        
        # Source the created function
        source status_updates/update_status.sh
        
        # Initial status with default values
        export STEPS_COMPLETED=0
        export FILE_COUNT=0
        export ISSUE_COUNT=0
        # Note: ANALYSIS_ID and REPO are set in $GITHUB_ENV in previous step
        # They will be available as env vars in this step
        update_analysis_status "queued" "Analysis request received" "initializing"
    
    - name: Clone and analyze repository
      run: |
        set -euo pipefail
        # Set workflow variables
        export WORKFLOW_RUN_ID="${{ github.run_id }}"
        export WORKFLOW_RUN_URL="${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
        
        # Source the update script using absolute path
        source $GITHUB_WORKSPACE/status_updates/update_status.sh
        
        # Initialize variables for this step
        export STEPS_COMPLETED=${STEPS_COMPLETED:-0}
        export FILE_COUNT=${FILE_COUNT:-0}
        export ISSUE_COUNT=${ISSUE_COUNT:-0}
        
        # Check if repository is provided
        if [ -z "${REPO}" ]; then
          echo "‚ùå Error: No repository provided!"
          update_analysis_status "failed" "No repository provided" "failed"
          exit 1
        fi
        
        echo "üîç Analyzing repository: ${REPO}"
        update_analysis_status "running" "Starting repository analysis for ${REPO}" "cloning"
        
        # Get default branch if not specified
        if [ -z "${BRANCH}" ] || [ "${BRANCH}" = "main" ] || [ "${BRANCH}" = "" ]; then
          DEFAULT_BRANCH=$(curl -s https://api.github.com/repos/${REPO} | jq -r .default_branch || echo "main")
          if [ "$DEFAULT_BRANCH" != "null" ] && [ -n "$DEFAULT_BRANCH" ]; then
            BRANCH=$DEFAULT_BRANCH
          else
            BRANCH="main"
          fi
        else
          BRANCH="${BRANCH}"
        fi
        
        echo "üìå Branch: $BRANCH"
        # Handle empty or invalid MAX_FILES
        if [ -z "${MAX_FILES}" ]; then
          MAX_FILES=500
        else
          MAX_FILES=$(echo "${MAX_FILES}" | cut -d. -f1)
        fi
        # Ensure MAX_FILES is a valid number
        if ! [[ "$MAX_FILES" =~ ^[0-9]+$ ]]; then
          MAX_FILES=500
        fi
        echo "MAX_FILES_INT=$MAX_FILES" >> $GITHUB_ENV
        echo "üìÑ Max files: $MAX_FILES"
        
        # Clone target repository
        git clone --depth 1 --branch $BRANCH \
          https://github.com/${REPO}.git target-repo || {
          echo "‚ùå Failed to clone repository ${REPO} (branch: $BRANCH)"
          echo "Please check:"
          echo "  - Repository exists and is public"
          echo "  - Branch name is correct"
          exit 1
        }
        
        # Get commit SHA in subshell
        COMMIT_SHA=$(cd target-repo && git rev-parse --short HEAD)
        echo "COMMIT_SHA=${COMMIT_SHA}" >> $GITHUB_ENV
        
        # Log repository info
        echo "üì¶ Successfully cloned: ${REPO}"
        echo "üîñ Commit: ${COMMIT_SHA}"
        echo "üìÅ Current directory: $(pwd)"
        echo "üìä Repository structure:"
        ls -la | head -10
        
        # Update status after successful clone
        export STEPS_COMPLETED=1
        update_analysis_status "running" "Repository cloned successfully, searching for C++ files..." "searching"
        
        # Find C++ files
        echo "üîé Searching for C++ files..."
        echo "üìÅ Current directory contents:"
        ls -la | head -10
        
        # Use find with error handling
        find . -type f \( -name "*.cpp" -o -name "*.cc" -o -name "*.cxx" -o -name "*.c" -o -name "*.h" -o -name "*.hpp" \) \
          -not -path "*/build/*" \
          -not -path "*/.git/*" \
          -not -path "*/vendor/*" \
          -not -path "*/third_party/*" \
          2>/dev/null | head -n $MAX_FILES > cpp_files.txt || true
        
        FILE_COUNT=$(wc -l < cpp_files.txt | tr -d ' ')
        echo "FILE_COUNT=${FILE_COUNT}" >> $GITHUB_ENV
        export FILE_COUNT
        
        if [ ${FILE_COUNT} -eq 0 ]; then
          echo "‚ùå No C++ files found in repository"
          echo "üìù Repository structure:"
          find . -type f -name "*.*" | grep -E "\.(cpp|cc|cxx|c|h|hpp)$" | head -20 || echo "No C++ files found"
          echo ""
          echo "üìÇ All files in repository (first 20):"
          find . -type f | head -20
          echo ""
          echo "‚ÑπÔ∏è Make sure the repository contains C++ source files with extensions: .cpp, .cc, .cxx, .c, .h, .hpp"
          echo "‚ÑπÔ∏è Excluded paths: */build/*, */.git/*, */vendor/*, */third_party/*"
          
          # Update status before exiting
          export STEPS_COMPLETED=2
          update_analysis_status "failed" "No C++ files found in repository" "failed"
          exit 1
        fi
        
        echo "‚úÖ Found ${FILE_COUNT} C++ files to analyze"
        echo "üìù First 10 files:"
        head -10 cpp_files.txt
        
        # Update status after finding files
        export STEPS_COMPLETED=2
        update_analysis_status "running" "Found ${FILE_COUNT} C++ files, starting static analysis..." "analyzing"
        
        # Run cppcheck
        echo "üîç Running cppcheck analysis..."
        cppcheck \
          --enable=all \
          --inconclusive \
          --suppress=missingIncludeSystem \
          --std=c++17 \
          --xml \
          --xml-version=2 \
          --file-list=cpp_files.txt \
          -j $(nproc) \
          2> ../cppcheck-results.xml || {
          echo "‚ö†Ô∏è CPPCheck returned non-zero exit code (this is normal if issues were found)"
        }
        
        cd ..
        
        # Debug: Show current directory structure
        echo "üìÅ Current directory: $(pwd)"
        echo "üìÇ Directory contents:"
        ls -la | head -10
        echo "üìÇ Looking for xml2json-simple.py:"
        find . -name "xml2json-simple.py" -type f 2>/dev/null || echo "Not found"
        
        # Verify XML output
        echo "üìÑ Checking cppcheck XML output..."
        if [ ! -f cppcheck-results.xml ]; then
          echo "‚ùå cppcheck-results.xml not found!"
          exit 1
        fi
        
        XML_SIZE=$(stat -c%s cppcheck-results.xml 2>/dev/null || stat -f%z cppcheck-results.xml 2>/dev/null || echo "0")
        echo "üìä XML file size: ${XML_SIZE} bytes"
        
        if [ ${XML_SIZE} -lt 100 ]; then
          echo "‚ö†Ô∏è Warning: XML file seems too small"
          echo "üìù XML content:"
          cat cppcheck-results.xml
        fi
        
        # Convert to JSON
        echo "üîÑ Converting XML to JSON..."
        # Debug: Show paths before running the script
        echo "üìÅ Working directory: $(pwd)"
        echo "üìÇ Script location check:"
        ls -la cppcheck-studio/xml2json-simple.py || echo "Script not found at expected location"
        
        # Run the script with the correct path
        python3 cppcheck-studio/xml2json-simple.py cppcheck-results.xml > analysis.json || {
          echo "‚ùå Failed to convert XML to JSON"
          echo "üìù XML content (first 500 chars):"
          head -c 500 cppcheck-results.xml
          exit 1
        }
        
        # Verify JSON output
        if [ ! -f analysis.json ]; then
          echo "‚ùå analysis.json not found!"
          exit 1
        fi
        
        JSON_SIZE=$(stat -c%s analysis.json 2>/dev/null || stat -f%z analysis.json 2>/dev/null || echo "0")
        echo "üìä JSON file size: ${JSON_SIZE} bytes"
        
        # Get issue count safely using inline Python
        ISSUE_COUNT=$(python3 -c "import json; data={'issues':[]}; exec(\"try:\\n    with open('analysis.json','r') as f: data=json.load(f)\\nexcept: pass\"); print(len(data.get('issues',[])))" || echo "0")
        
        echo "ISSUE_COUNT=${ISSUE_COUNT}" >> $GITHUB_ENV
        export ISSUE_COUNT
        echo "üìä Found ${ISSUE_COUNT} issues"
        
        if [ ${ISSUE_COUNT} -eq 0 ]; then
          echo "‚ö†Ô∏è No issues found. This could mean:"
          echo "  - The code has no issues (great!)"
          echo "  - CPPCheck couldn't analyze the files"
          echo "  - There was an error in processing"
          echo "üìù First 500 chars of analysis.json:"
          head -c 500 analysis.json
        fi
        
        # Update status after analysis
        export STEPS_COMPLETED=3
        update_analysis_status "running" "Analysis complete: found ${ISSUE_COUNT} issues in ${FILE_COUNT} files" "processing"
        
        # Add code context
        echo "üìù Adding code context..."
        # Get repository path
        REPO_PATH=$(cd target-repo && pwd)
        echo "REPO_PATH=${REPO_PATH}" >> $GITHUB_ENV
        echo "üìÅ Repository path: $REPO_PATH"
        
        # Show first few files for debugging using inline Python
        echo "üìÑ Sample files from analysis:"
        python3 -c "import json; exec(\"try:\\n    with open('../analysis.json','r') as f: data=json.load(f)\\n    files=set(issue.get('file','') for issue in data.get('issues',[])[:10])\\n    for f in list(files)[:5]:\\n        if f: print(f'  - {f}')\\nexcept: print('Could not list files')\")" || echo "Could not list files"
        
        # Run from parent directory to maintain consistent paths
        cd ..
        python3 cppcheck-studio/add-code-context.py analysis.json analysis-with-context.json --base-path "$REPO_PATH" || {
          echo "‚ö†Ô∏è Failed to add code context, using original analysis"
          cp analysis.json analysis-with-context.json
        }

    - name: Push status updates to GitHub Pages
      if: github.ref == 'refs/heads/main'
      run: |
        set -euo pipefail
        # Clone the repo with authentication  
        git clone https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git status-repo
        cd status-repo
        
        # Create status directory
        mkdir -p docs/api/status
        
        # Copy latest status if it exists
        if [ -f "$GITHUB_WORKSPACE/status_updates/current_status.json" ]; then
          cp "$GITHUB_WORKSPACE/status_updates/current_status.json" docs/api/status/${ANALYSIS_ID}.json
        else
          # Fallback status
          printf '%s\n' \
            '{' \
            '  "analysis_id": "'"${ANALYSIS_ID}"'",' \
            '  "repository": "'"${REPO}"'",' \
            '  "status": "running",' \
            '  "workflow_run_id": "'"${{ github.run_id }}"'",' \
            '  "workflow_run_url": "'"${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"'",' \
            '  "updated_at": "'"$(date -u +%Y-%m-%dT%H:%M:%SZ)"'",' \
            '  "message": "Processing analysis...",' \
            '  "progress": {' \
            '    "steps_completed": 3,' \
            '    "total_steps": 5,' \
            '    "current_step": "processing"' \
            '  }' \
            '}' > docs/api/status/${ANALYSIS_ID}.json
        fi
        
        # Commit and push status update
        git config user.name "GitHub Actions"
        git config user.email "actions@github.com"
        git add docs/api/status/
        git commit -m "Update analysis status for ${REPO} (#${ANALYSIS_ID})" || echo "No changes"
        git push || echo "Push failed, likely concurrent update"
        
        cd ..
        
    - name: Generate dashboard
      run: |
        set -euo pipefail
        # Set workflow variables
        export WORKFLOW_RUN_ID="${{ github.run_id }}"
        export WORKFLOW_RUN_URL="${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
        
        source $GITHUB_WORKSPACE/status_updates/update_status.sh
        
        # Variables are already in environment from previous steps
        export STEPS_COMPLETED=4
        update_analysis_status "running" "Generating interactive dashboard..." "generating"
        
        # Create output directory
        mkdir -p output
        
        # Debug: Show current directory and files
        echo "üìÅ Current directory: $(pwd)"
        echo "üìÇ Files in current directory:"
        ls -la | head -10
        
        # Verify analysis files exist
        if [ ! -f analysis-with-context.json ]; then
          echo "‚ùå analysis-with-context.json not found!"
          if [ -f analysis.json ]; then
            echo "‚ö†Ô∏è Using analysis.json instead"
            cp analysis.json analysis-with-context.json
          else
            echo "‚ùå No analysis files found!"
            exit 1
          fi
        fi
        
        echo "üìä Analysis file info:"
        ls -la analysis-with-context.json
        ANALYSIS_SIZE=$(stat -c%s analysis-with-context.json 2>/dev/null || stat -f%z analysis-with-context.json 2>/dev/null || echo "0")
        echo "üìä Analysis file size: ${ANALYSIS_SIZE} bytes"
        
        # Check if analysis has issues
        echo "üìù First few issues:"
        head -c 1000 analysis-with-context.json
        
        # Generate dashboard
        PROJECT_NAME="${REPO}"
        DASHBOARD_GENERATED=false
        
        # Check which generator to use (prioritize optimized dashboard)
        if [ -f cppcheck-studio/generate/generate-optimized-dashboard.py ]; then
          echo "üé® Using optimized dashboard generator (developer-focused workflow)..."
          python3 cppcheck-studio/generate/generate-optimized-dashboard.py \
            analysis-with-context.json \
            output/dashboard-${ANALYSIS_ID}.html || {
            echo "‚ö†Ô∏è Optimized generator failed, trying alternatives..."
          }
        fi
        
        # Check if dashboard was generated
        if [ -f output/dashboard-${ANALYSIS_ID}.html ]; then
          DASHBOARD_GENERATED=true
          echo "‚úÖ Dashboard generated successfully"
        elif [ -f cppcheck-studio/generate/generate-simple-dashboard.py ]; then
          echo "üé® Trying simple dashboard generator..."
          python3 cppcheck-studio/generate/generate-simple-dashboard.py \
            analysis-with-context.json \
            output/dashboard-${ANALYSIS_ID}.html && DASHBOARD_GENERATED=true
        fi
        
        # Last resort - create minimal dashboard
        if [ "$DASHBOARD_GENERATED" = "false" ]; then
          echo "‚ö†Ô∏è All generators failed, creating minimal dashboard..."
          printf '%s\n' \
            '<!DOCTYPE html>' \
            '<html>' \
            '<head>' \
            '    <title>CPPCheck Analysis - '"${REPO}"'</title>' \
            '    <style>' \
            '        body { font-family: Arial, sans-serif; margin: 20px; }' \
            '        .header { background: #f0f0f0; padding: 20px; border-radius: 5px; }' \
            '        .error { color: red; }' \
            '        .info { color: blue; }' \
            '    </style>' \
            '</head>' \
            '<body>' \
            '    <div class="header">' \
            '        <h1>CPPCheck Analysis Results</h1>' \
            '        <p><strong>Repository:</strong> '"${REPO}"'</p>' \
            '        <p><strong>Analysis ID:</strong> '"${ANALYSIS_ID}"'</p>' \
            '        <p><strong>Files Analyzed:</strong> '"${FILE_COUNT}"'</p>' \
            '        <p><strong>Issues Found:</strong> '"${ISSUE_COUNT}"'</p>' \
            '    </div>' \
            '    <div class="error">' \
            '        <h2>Dashboard Generation Error</h2>' \
            '        <p>The full interactive dashboard could not be generated. Please check the workflow logs for details.</p>' \
            '        <p>Raw analysis data is available in the workflow artifacts.</p>' \
            '    </div>' \
            '</body>' \
            '</html>' > output/dashboard-${ANALYSIS_ID}.html
        fi
        
        # Verify dashboard was created
        if [ ! -f output/dashboard-${ANALYSIS_ID}.html ]; then
          echo "‚ùå Failed to create dashboard!"
          exit 1
        fi
        
        DASHBOARD_SIZE=$(stat -c%s output/dashboard-${ANALYSIS_ID}.html 2>/dev/null || stat -f%z output/dashboard-${ANALYSIS_ID}.html 2>/dev/null || echo "0")
        echo "üìä Dashboard file size: ${DASHBOARD_SIZE} bytes"
        
        # Generate summary (with error handling)
        echo "üìù Generating summary..."
        python3 cppcheck-studio/scripts/generate-summary.py analysis.json > output/summary.txt || {
          echo "‚ö†Ô∏è Summary generation failed"
          echo "No summary available" > output/summary.txt
        }
        
        # Generate detailed report (with error handling)
        echo "üìù Generating detailed report..."
        python3 cppcheck-studio/scripts/generate-detailed-report.py analysis.json > output/report.md || {
          echo "‚ö†Ô∏è Report generation failed"
          echo "# Report Generation Failed" > output/report.md
          echo "Please check the analysis.json file" >> output/report.md
        }
        
        # Create metadata with issue breakdown
        echo "üìù Creating metadata with issue breakdown..."
        
        # Get issue breakdown
        ISSUE_BREAKDOWN=$(python3 cppcheck-studio/scripts/extract-issue-breakdown.py analysis.json || echo '{"total":0,"error":0,"warning":0,"style":0,"performance":0,"portability":0,"information":0}')
        echo "üìä Issue breakdown: $ISSUE_BREAKDOWN"
        
        # Create gallery-compatible metadata
        jq -n \
          --arg id "${ANALYSIS_ID}" \
          --arg repo "${REPO}" \
          --arg branch "${BRANCH}" \
          --arg commit "${COMMIT_SHA}" \
          --arg timestamp "$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
          --argjson files "${FILE_COUNT}" \
          --argjson issues_json "${ISSUE_BREAKDOWN}" \
          --argjson issue_count "${ISSUE_COUNT}" \
          --argjson max_files "${MAX_FILES_INT}" \
          --arg dashboard_url "https://jerryzhao173985.github.io/cppcheck-studio/results/${ANALYSIS_ID}/index.html" \
          --argjson dashboard_size "${DASHBOARD_SIZE}" \
          --argjson analysis_size "${ANALYSIS_SIZE}" \
          '{
            analysis_id: $id,
            repository: $repo,
            branch: $branch,
            commit: $commit,
            timestamp: $timestamp,
            filesAnalyzed: $files,
            files_analyzed: $files,
            issues: $issues_json,
            issues_found: $issue_count,
            max_files: $max_files,
            dashboardUrl: $dashboard_url,
            dashboard_url: $dashboard_url,
            dashboard_size: $dashboard_size,
            analysis_size: $analysis_size
          }' > output/metadata.json
        
        echo "‚úÖ Dashboard generation complete!"

    - name: Upload results as artifacts
      uses: actions/upload-artifact@v4
      with:
        name: analysis-${{ github.event.inputs.analysis_id || github.event.client_payload.analysis_id || 'generated' }}
        path: |
          output/dashboard-*.html
          output/summary.txt
          output/report.md
          output/metadata.json
          analysis.json
          analysis-with-context.json

    - name: Update analysis results and status
      if: github.ref == 'refs/heads/main'
      run: |
        set -euo pipefail
        # Clone the repo with authentication
        git clone https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git deploy-repo
        cd deploy-repo
        
        # Create directories
        mkdir -p docs/results/${ANALYSIS_ID}
        mkdir -p docs/api/status
        mkdir -p docs/api/analyses
        
        # Copy dashboard and metadata
        cp ../output/dashboard-${ANALYSIS_ID}.html docs/results/${ANALYSIS_ID}/index.html
        cp ../output/metadata.json docs/results/${ANALYSIS_ID}/
        cp ../output/metadata.json docs/api/analyses/${ANALYSIS_ID}.json
        
        # Create final status with full details
        ISSUES_JSON=$(cat ../output/metadata.json | jq -c .issues || echo '{"total":0}')
        jq -n \
          --arg id "${ANALYSIS_ID}" \
          --arg repo "${REPO}" \
          --arg status "completed" \
          --arg step "completed" \
          --arg workflow_run_id "${{ github.run_id }}" \
          --arg workflow_run_url "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}" \
          --arg completed_at "$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
          --argjson files "${FILE_COUNT}" \
          --argjson issue_count "${ISSUE_COUNT}" \
          --argjson issues_obj "${ISSUES_JSON}" \
          --arg dashboard_url "https://jerryzhao173985.github.io/cppcheck-studio/results/${ANALYSIS_ID}/index.html" \
          '{
            analysis_id: $id,
            repository: $repo,
            status: $status,
            step: $step,
            workflow_run_id: $workflow_run_id,
            workflow_run_url: $workflow_run_url,
            completed_at: $completed_at,
            files_analyzed: $files,
            filesAnalyzed: $files,
            issues_found: $issue_count,
            issues: $issues_obj,
            dashboard_url: $dashboard_url,
            dashboardUrl: $dashboard_url,
            progress: {
              steps_completed: 5,
              total_steps: 5,
              current_step: "completed"
            }
          }' > docs/api/status/${ANALYSIS_ID}.json
        
        # Update gallery
        if [ -f docs/api/gallery.json ]; then
          jq --argjson new "$(cat docs/api/analyses/${ANALYSIS_ID}.json)" \
            '.analyses = ([$new] + .analyses | unique_by(.analysis_id) | .[0:50])' \
            docs/api/gallery.json > temp.json
          mv temp.json docs/api/gallery.json
        else
          jq -n --argjson metadata "$(cat docs/api/analyses/${ANALYSIS_ID}.json)" \
            '{analyses: [$metadata]}' > docs/api/gallery.json
        fi
        
        # Commit and push
        git config user.name "GitHub Actions"
        git config user.email "actions@github.com"
        git add docs/
        git commit -m "Add analysis results for ${REPO} (#${ANALYSIS_ID})" || echo "No changes"
        git push
        
        cd ..

    - name: Send callback if provided
      if: env.CALLBACK_URL != ''
      run: |
        set -euo pipefail
        # Send results to callback URL
        curl -X POST ${CALLBACK_URL} \
          -H "Content-Type: application/json" \
          -d @output/metadata.json \
          || echo "Failed to send callback"

    - name: Create enhanced job summary
      if: always()
      run: |
        set -euo pipefail
        DASHBOARD_URL="https://jerryzhao173985.github.io/cppcheck-studio/results/${ANALYSIS_ID}/index.html"
        
        # Create job summary using script
        bash cppcheck-studio/scripts/create-job-summary.sh \
          "${DASHBOARD_URL}" \
          "${REPO}" \
          "${BRANCH}" \
          "${COMMIT_SHA}" \
          "${ANALYSIS_ID}" \
          "${FILE_COUNT:-0}" \
          "${ISSUE_COUNT:-0}" | \
        sed -e "s|DASHBOARD_URL_PLACEHOLDER|${DASHBOARD_URL}|g" \
            -e "s|REPO_PLACEHOLDER|${REPO}|g" \
            -e "s|BRANCH_PLACEHOLDER|${BRANCH}|g" \
            -e "s|COMMIT_PLACEHOLDER|${COMMIT_SHA}|g" \
            -e "s|ANALYSIS_ID_PLACEHOLDER|${ANALYSIS_ID}|g" \
            -e "s|FILE_COUNT_PLACEHOLDER|${FILE_COUNT:-0}|g" \
            -e "s|ISSUE_COUNT_PLACEHOLDER|${ISSUE_COUNT:-0}|g" \
            -e "s|ARTIFACTS_URL_PLACEHOLDER|${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}#artifacts|g" \
            -e "s|WORKFLOW_URL_PLACEHOLDER|${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|g" \
            -e "s|WORKFLOW_RUN_ID|${{ github.run_id }}|g" \
            -e "s|EVENT_NAME|${{ github.event_name }}|g" \
            -e "s|RUNNER_OS|${{ runner.os }}|g" \
            -e "s|COMPLETED_AT|$(date -u +%Y-%m-%dT%H:%M:%SZ)|g" \
        >> $GITHUB_STEP_SUMMARY
